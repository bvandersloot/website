<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<link rel="stylesheet" href="/css/custom.css">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="keywords" content="Benjamin VanderSloot">
		<meta name="author" content="Benjamin VanderSloot">
	  	<meta name="google-site-verification" content="ikjDR_PBmxNlGYmzIGqhL8TjMtiUhgc1nKUTJlbdHZI" />	
		<title>Benjamin VanderSloot</title>
	</head>
	<body>
		<div class="main">
			
<h2>
 benvds blog v3 
</h2>

<h3>Reflecting on: Exposing the Big Con: The False Promise of Artificial Intelligence</h3>

<div class="blog-body">
<p>First off, thanks <a href="https://socialistproject.ca/author/marty-hart-landsberg/">Martin Hart-Landsberg</a>  for <a href="https://socialistproject.ca/2025/03/exposing-the-big-con/">the article</a>!</p>
<p>This article starts spinning from a <a href="https://www.businessinsider.com/elon-musk-humanoid-robots-ai-benefits-optimus-2025-2">couple</a> of <a href="https://www.bloomberg.com/news/newsletters/2025-02-16/apple-and-meta-are-set-to-battle-over-new-area-humanoid-robots-m77mwid3">articles</a> in the business press about tech companies talking a big game about their humanoid robotics programs. Presumably the tech companies needed to make replacing workers more literal for investors. From there, the article pivots to reasonable critiques of LLMs and a nice <a href="https://www.windowscentral.com/software-apps/microsoft-copilot-struggles-to-discern-facts-from-opinions-bbc-study">example from the BBC</a> of LLMs not being good at the whole “fact” thing. And they are actually getting more prone to make stuff up in an attempt to seem more useful. Despite Silicon Valley’s leadership’s boldest claims, the IMF doesn’t see the needle of productivity moving as much as it needs to to justify the expenses in training these models. Then the article closes out by helpfully stating the obvious that we shouldn’t trust these things and need to organize our workplaces.</p>
<p>One particularly nice detail is the suggestion that  “we must always ensure that humans have the ability to review and, when necessary, override AI decisions.” I love that and think it gets started towards some of the algorithmic justice thoughts that predate LLMs. My brain went to <a href="https://www.goodreads.com/book/show/34964830-automating-inequality">Automating Inequality by Virginia Eubanks</a>, but that isn’t all.</p>
<p>I love the critiques of technical limitations of LLMs. I think comparing them to autocomplete or a blurry JPEG of the training data could explain just a bit more technically while staying approachable. But as written it strikes a good balance in making it clear that thinking is beyond the grasp of LLMs.</p>
<p>The whole thing is a reminder to not drink the kool aid in the current mania, even if drinking it might make some criticisms of Capital easier.</p>
<p>What worries me, looking back from the end of the article, is still the philosophical gap between the tech critic and tech optimist. I am not sure how to reach across that gap. Consider one example use of LLMs mentioned in passing in the article (and with a connection to the roots of AI ethics): mental health counseling. Is it necessary that the person you seek advice from be a human, with human experiences, and understand what I said and felt? For any clinical results, probably not. But something about not having that connection sucks the blood from living life for me. While the piece hits for me, I’m not sure if it does anything for someone who is going to reflect on it later with their <a href="https://www.mindsera.com/">AI-powered Tom Riddle’s Diary</a>.</p>


<hr/>
<p class="marginless">
 Benjamin VanderSloot, 27 March 2025
</p>
</div>



		</div>
	</body>
</html>
